<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Report - Predicting Yak Scores based on text and temporal features</title>
  <style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet" href="static/github-markdown.css">
<style>
    .markdown-body {
        min-width: 200px;
        max-width: 790px;
        margin: 0 auto;
        padding: 30px;
    }
</style>
</head>
<body>
<article class="markdown-body">
<div id="header">
<h1 class="title">Report - Predicting Yak Scores based on text and temporal features</h1>
</div>
<p>EECS 349, Northwestern University, Spring 2015</p>
<p>Matthew Heston <script type="text/javascript">
<!--
h='&#x75;&#46;&#110;&#x6f;&#114;&#116;&#104;&#x77;&#x65;&#x73;&#116;&#x65;&#114;&#110;&#46;&#x65;&#100;&#x75;';a='&#64;';n='&#104;&#x65;&#x73;&#116;&#x6f;&#110;';e=n+a+h;
document.write('<a h'+'ref'+'="ma'+'ilto'+':'+e+'">'+e+'<\/'+'a'+'>');
// -->
</script><noscript>&#104;&#x65;&#x73;&#116;&#x6f;&#110;&#32;&#x61;&#116;&#32;&#x75;&#32;&#100;&#x6f;&#116;&#32;&#110;&#x6f;&#114;&#116;&#104;&#x77;&#x65;&#x73;&#116;&#x65;&#114;&#110;&#32;&#100;&#x6f;&#116;&#32;&#x65;&#100;&#x75;</noscript></p>
<p>Jeremy Foote <script type="text/javascript">
<!--
h='&#x75;&#46;&#110;&#x6f;&#114;&#116;&#104;&#x77;&#x65;&#x73;&#116;&#x65;&#114;&#110;&#46;&#x65;&#100;&#x75;';a='&#64;';n='&#106;&#100;&#102;&#x6f;&#x6f;&#116;&#x65;';e=n+a+h;
document.write('<a h'+'ref'+'="ma'+'ilto'+':'+e+'">'+e+'<\/'+'a'+'>');
// -->
</script><noscript>&#106;&#100;&#102;&#x6f;&#x6f;&#116;&#x65;&#32;&#x61;&#116;&#32;&#x75;&#32;&#100;&#x6f;&#116;&#32;&#110;&#x6f;&#114;&#116;&#104;&#x77;&#x65;&#x73;&#116;&#x65;&#114;&#110;&#32;&#100;&#x6f;&#116;&#32;&#x65;&#100;&#x75;</noscript></p>
<h2 id="task-overview">Task Overview</h2>
<p>Our task, as <a href="abstract.html">explained</a>, was to predict the score that a yak posted on Yik Yak would receive. Yaks that reach a score of -5 are automatically deleted, making the lowest possible score -4. There is no upper limit on scores. Rather than work directly with the continuous score value, we discretize scores by using the following rules:</p>
<ul>
<li>All yaks with a score of less than 0 form a category.</li>
<li>All yaks with scores higher than the 90th percentile form a category.</li>
<li>The rest of the yaks then form a middle category.</li>
</ul>
<p>Our task is then to predict which of these three classes a yak will fall into. These class distributions are generally not uniform, with most yaks falling into the &quot;middle&quot; category, and much smaller percentage falling into the negative and highly upvoted categories. We therefore use F1 as an evaluation metric throughout this paper.</p>
<h2 id="data-collection">Data Collection</h2>
<p>Data was collected from September 2014 to February 2015. A script ran every 1 hour and retrieved the last 100 yaks posted to 35 different campuses. In the experiments and analysis listed below, we use data from only two of these campuses: Northwestern University and Florida State University. Northwestern was chosen as we are affiliated with the university and assumed we therefore might have some intuition about why certain features might be associated with different scores. Florida State was chosen as it by far produced the most yaks of all the campuses in our collection, and we thought it would be valuable to test our methods using a campus with a large number of documents for training. For each yak collected, we have the message text, the time it was posted, and the score of the yak as of the last time our script collected data about the yak.</p>
<p>For our initial experiments, we had 26,628 yaks from Northwestern, and 112,518 yaks from FSU. Our cross validation results use these yaks. In addition, we have 7687 heldout Northwestern yaks and 33,584 heldout yaks FSU yaks for testing on unseen data.</p>
<h2 id="feature-extraction">Feature Extraction</h2>
<p>Our primary feature set consisted of bag of words representations of the yaks themselves. We trained models using both unigram and bigram representations. In both cases, we use a binary bag of words representation, in which each term is represented by a 0 or 1 indicating whether or not it appeared in the yak.</p>
<p>In addition bag of words, we derived other features. These include features based on the timestamp of the yak, including time of day (late night, early morning, etc) and day of week. We also included addition features about the text itself, including how many words were including the total number of characters in the yak, the number of words in the yak, and the mean word length.</p>
<h2 id="learning-algorithms">Learning Algorithms</h2>
<p>We trained both a logistic regression and a naive Bayes classifier using various configurations of the features listed above. We used the implementations of these algorithms in the Python library <a href="http://scikit-learn.org/">scikit-learn</a>.</p>
<p>Hyperparameter tuning was performed for logistic regression using the scikit-learn <code>gridsearch</code> module. Based on results from 5-fold cross-validation, selecting parameters based on F1 score, we decided on using L2 penalization in our logistic regression model and set the C value (inverse of regulization strength) to 0.1. It should be noted that all models in our parameter tuning performed fairly similarly, but this model did boost mean F1 score around 4% from the lowest performing configuration. We used default parameter settings for naive Bayes.</p>
<h2 id="final-results">Final Results</h2>
<h2 id="cross-validation">Cross Validation</h2>
<p>Results from 5-fold cross validation for each classifier, using different features, are shown in the following tables. The first table shows results for our corpus of Northwestern yaks. The second shows results for our corpus of Florida State yaks.</p>
<table>
  <tr>
    <th></th>
    <th>
Unigram Features
</th>
    <th>
Unigram + Bigram Features
</th>
    <th>
Unigram + Bigram + Time and Text Features
</th>
  </tr>
  <tr>
    <td>
Naive Bayes
</td>
    <td>
68.6%
</td>
    <td>
68.6%
</td>
    <td>
68.1%
</td>
  </tr>
  <tr>
    <td>
Logistic Regression
</td>
    <td>
68.9%
</td>
    <td>
69.2%
</td>
    <td>
69.2%
</td>
  </tr>
  <tr>
    <td colspan="4">
Note: Percentages reflect average F1 score from 5-fold cross validation.
</td>
  </tr>
</table>

<table>
  <tr>
    <th></th>
    <th>
Unigram Features
</th>
    <th>
Unigram + Bigram Features
</th>
    <th>
Unigram + Bigram + Time and Text Features
</th>
  </tr>
  <tr>
    <td>
Naive Bayes
</td>
    <td>
75.2%
</td>
    <td>
75.1%
</td>
    <td>
75.3%
</td>
  </tr>
  <tr>
    <td>
Logistic Regression
</td>
    <td>
76.0%
</td>
    <td>
76.0%
</td>
    <td>
76.1%
</td>
  </tr>
  <tr>
    <td colspan="4">
Note: Percentages reflect average F1 score from 5-fold cross validation.
</td>
  </tr>
</table>

<p>In both cases, both classifiers perform comparably. All of our feature sets also perform comparably. Adding bigrams does not improve results significantly over using only unigrams. Nor do our derived time and text features make a significant difference. We see better F1 scores with the classifiers trained on Florida State corpus, likely due to the relatively much larger collection of documents for training.</p>
<h2 id="testing-on-heldout-data">Testing on Heldout Data</h2>
<p>Training on all 26,628 Northwestern yaks and testing on our heldout 7,687 yaks resulted in an F1 score of 76.7%. Doing the same with our 112,518 FSU yaks and then testing on the heldout set of 33,584 yaks resulted in an F1 score of 74.7%. In both of these cases, we use our logistic regression classifier and unigra, bigram, and other other time and text features. The results here are comparable to our cross validation results, though the Northwestern classifier performed much better than expected given its cross validation results.</p>
<h2 id="top-features-for-each-classifier">Top Features for Each Classifier</h2>
<p>Below we present the top 20 terms associated with each class, for both Northwestern and Florida State.</p>
<p><strong>Florida State</strong></p>
<p><em>Negative</em><br />fartmonster<br />chris<br />lmfao<br />john<br />shrek<br />grindr<br />kik<br />justin<br />you if<br />sunny<br />jimmy<br />bradsmithfsu<br />the middle<br />go gators<br />salley<br />doebotz<br />nigga<br />middle<br />bit<br />page</p>
<p><em>Middle</em><br />it or<br />pack<br />bad<br />summer<br />away<br />dirac<br />that was<br />basketball<br />wishing<br />that is<br />traditions<br />never had<br />buddy<br />to just<br />girl to<br />fml<br />totally<br />finding<br />the one<br />crazy</p>
<p><em>Highly Positive</em><br />chiefs<br />petition<br />95<br />password<br />pregnant<br />gator<br />uf<br />pregnancy<br />years ago<br />fsu<br />strozier<br />college<br />upvote<br />publix<br />college where<br />99<br />unconquered<br />cancer<br />tallahassee</p>
<p><strong>Northwestern</strong></p>
<p><em>Negative</em><br />anyone<br />ass<br />ugh<br />women<br />south<br />zbt<br />in norris<br />cigs<br />that the<br />palestine<br />smoke<br />white<br />wet<br />suck<br />how can<br />me out<br />israel<br />cig<br />norris<br />cunt</p>
<p><em>Middle</em><br />wash<br />post<br />music<br />breakfast<br />less<br />what the<br />the fuck<br />forever<br />the first<br />could<br />time<br />mudd<br />asleep<br />outside<br />hours<br />beer<br />the day<br />you guys<br />when your<br />stress</p>
<p><em>Highly Upvoted</em><br />different<br />99<br />kappa<br />staying<br />his<br />chipotle<br />petition<br />is like<br />single<br />student<br />wearing<br />awkward<br />college<br />grade<br />degree<br />shark<br />date<br />degrees<br />gpa<br />northwestern</p>
<h2 id="discussion">Discussion</h2>
<p>It is first interesting to note that the addition of bigrams and our other derived features did not seem to boost performance that much in either of our classifiers. (Why might this be?)</p>
<p>Inspecting our predictive features is not always intuitive, though it is in some cases entertaining (&quot;Chipotle&quot; being highly predictive of being upvoted, for example.) Some examples are intuitive and illustrative, however. We see that certain offensive language is associated with being downvoted. School names or mascots, on the other hand, are associated with a yak doing well. While it's difficult to extrapolate too much from this, we can begin to see that in some cases, students may reward local and relevant posts while punishing offensive content, which is what we might expect. There are some surprising cases, however. &quot;Go gators&quot; is predictive of being downvoted while &quot;FSU&quot; is associated with upvotes. More qualitative analysis would be necessary to understand possibly differences here.</p>
</article>
</body>
</html>
